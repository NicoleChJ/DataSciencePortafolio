{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c785cfe2",
   "metadata": {},
   "source": [
    "# Ollama Webpage summarization\n",
    " Before executing the code, ensure that the initial setup outlined in the Readme.md file has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"   # 11434 port where ollama runs\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"  #Model type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b2a357",
   "metadata": {},
   "source": [
    "### Test: Verifying Connection with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list. Has the same format  used on OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]\n",
    "\n",
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba30d5c",
   "metadata": {},
   "source": [
    "There is 2 ways of running ollama package:\n",
    "\n",
    "- Direct HTTP call: you can send HTTP POST requests to the Ollama API\n",
    "- ollama python package: provides a higher-level interface to interact with the model, abstracting the direct HTTP requests\n",
    "- Using OpenAI python library to connect to Ollama\n",
    "\n",
    "This are making web requests locally from my box to my box. And it's connecting to the llama 3.2 model that's being served by llama, and running at localhost:11434\n",
    "\n",
    "\n",
    "<u> Recommendation:</u>\n",
    "- If you are primarily using Ollama: Go with ollama Python Package. Itâ€™s the cleanest and most idiomatic solution specifically designed for Ollama.\n",
    "- If you are using both OpenAI and Ollama APIs: Use OpenAI Library (OpenAI) with custom configuration. This provides a unified approach to handling both APIs.\n",
    "- For advanced or custom needs: Use Direct HTTP Calls (requests.post) for full control and flexibility, especially if you need to customize headers, handle retries, or debug raw API responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c9797e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "### Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402d5686-4e76-4110-b65a-b3906c35c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt Setup:\n",
    "\n",
    "\"\"\"We need to create a structure the APIs understand:\n",
    "#[ {\"role\": \"system\", \"content\": \"system message goes here\"},\n",
    "#  {\"role\": \"user\", \"content\": \"user message goes here\"} ] \"\"\"\n",
    "\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\"\n",
    "\n",
    "\n",
    "def user_prompt_for(website): # A function that writes a User Prompt that asks for summaries of websites:\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt\n",
    "\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "319fca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping \n",
    "\n",
    "\n",
    "class Website: # A class to represent a Webpage, analyize the webpage and extract the information\n",
    "\n",
    "    def __init__(self, url): # Create this Website object from the given url using the BeautifulSoup librarY\n",
    "        \n",
    "        self.url = url\n",
    "\n",
    "        try: #Handling errors\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raises HTTPError if the response code is not 200\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            self.title = soup.title.string if soup.title else \"No title found\"\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]): #Clean some unseful things\n",
    "                irrelevant.decompose()\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching the URL: {e}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b273a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Website Class Testing\n",
    "\n",
    "try:\n",
    "    law = Website(\"https://www.law.com/?slreturn=20250125165349\")\n",
    "    print(law.title)\n",
    "    print(law.text)\n",
    "    print(user_prompt_for(law))\n",
    "    messages_for(law) #Test the summary of the webpage\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Website object: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef290970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Execution\n",
    "import ollama\n",
    "\n",
    "def summarize(url):  # And now: call the  API, to do the summarization\n",
    "    website = Website(url) # Web scraping\n",
    "    try:\n",
    "        response = ollama.chat(model=MODEL, messages=messages_for(website))  # LLM inference\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        print(f\"Error during LLM API call: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "summarize(\"https://www.law.com/?slreturn=20250125165349\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070b9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_summary(url):# A function to display this nicely in the Jupyter output, using markdown\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))\n",
    "\n",
    "display_summary(\"https://www.law.com/?slreturn=20250125165349\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
