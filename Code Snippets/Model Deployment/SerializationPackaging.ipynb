{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization/ Save the Model\n",
    "Before deployment, the trained model must be saved in a format that allows easy loading and inference. Common formats include:\n",
    "- Pickle (.pkl) – Simple models in Python environments.\n",
    "- Joblib (.joblib) -  Efficient for sklearn-based models, optimized for large objects.\n",
    "- ONNX (.onnx) – Cross-platform deployment (models work outside Python)., optimized for fast inference in cloud and edge environments.\n",
    "- .json/.yaml -  Text-based storage format, useful for model structure saving, so it can be rebuilt easily.\n",
    "- TensorFlow SavedModel / HDF5 (.h5) – Used for deep learning models.\n",
    "\n",
    "## Pipeline Packaging\n",
    "Once you've already trained multiple models, selected the best one, and finalized your preprocessing steps, you go back and create a unified pipeline that includes both preprocessing and the selected model. This ensures that training and inference use the same transformations and prevents issues like applying different scaling methods at different stages.\n",
    "\n",
    "- Common Preprocessing Steps Required at Inference Time:\n",
    "    - Feature scaling/normalization\n",
    "    - One-hot encoding\n",
    "    - Feature transformations\n",
    "    - Handling missing values\n",
    "    - Text vectorization\n",
    "    - Dimensionality reduction\n",
    "    - Any custom feature engineering\n",
    "\n",
    "- Implementation Methods:\n",
    "    - Scikit-learn Pipelines\n",
    "    - TensorFlow Transform\n",
    "    - Feature stores\n",
    "    - Custom preprocessing modules bundled with the model\n",
    "\n",
    "- Key Takeaways:\n",
    "    - You don’t retrain the model – you load the pre-trained model and integrate it into the pipeline.\n",
    "    - Ensure all preprocessing steps are the same as used during training.\n",
    "    - Fit only preprocessing steps before adding them to the pipeline.\n",
    "    - Save the complete pipeline so that inference can be run in a single step later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# 1️⃣ Load the selected model (previously trained)\n",
    "best_model = joblib.load(\"best_xgboost_model.joblib\")\n",
    "\n",
    "# 2️⃣ Define the preprocessing steps based on what was used during training\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('scaler', StandardScaler(), ['amount', 'transactions']),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'), ['category'])\n",
    "])\n",
    "\n",
    "# 3️⃣ Create the final pipeline (preprocessing + final model)\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', best_model)  # Use the already trained model\n",
    "])\n",
    "\n",
    "# 4️⃣ Save the final pipeline\n",
    "joblib.dump(final_pipeline, \"fraud_detection_pipeline.joblib\")\n",
    "\n",
    "print(\"Final pipeline saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and Running Inference:\n",
    "\n",
    "Remember ensure the new input data is structured like the training data (same column names and types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ Load the saved pipeline\n",
    "pipeline = joblib.load(\"fraud_detection_pipeline.joblib\")\n",
    "\n",
    "# 2️⃣ Prepare new data (it should match the format used during training)\n",
    "new_data = pd.DataFrame({\n",
    "    'amount': [500.0],\n",
    "    'transactions': [10],\n",
    "    'category': ['electronics']\n",
    "})\n",
    "\n",
    "# 3️⃣ Run inference (both preprocessing & model prediction are handled in one step)\n",
    "prediction = pipeline.predict(new_data)\n",
    "\n",
    "print(f\"Predicted class: {prediction[0]}\")\n",
    "\n",
    "\n",
    "# If you need confidence scores, use .predict_proba() instead:\n",
    "\n",
    "probabilities = pipeline.predict_proba(new_data)\n",
    "print(f\"Fraud Probability: {probabilities[0][1]:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# 1️⃣ Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "feature_names = load_iris().feature_names  # Get feature names for reference\n",
    "\n",
    "# Introduce some missing values (for demonstration purposes)\n",
    "np.random.seed(42)\n",
    "X[np.random.randint(0, X.shape[0], 5), np.random.randint(0, X.shape[1], 5)] = np.nan\n",
    "\n",
    "# 2️⃣ Split dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3️⃣ Define a Custom Transformer for manual feature manipulation\n",
    "class CustomFeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # No fitting required for transformation\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()  # Ensure we don't modify the original data\n",
    "        \n",
    "        # Apply manual transformations:\n",
    "        new_feature = np.sqrt(np.abs(X[:, 0] * X[:, 1]))  # Example: Multiply first two columns and apply sqrt. Creates a new feature \n",
    "        new_feature = new_feature.reshape(-1, 1)  # Reshape to add as a new feature. \n",
    "        \n",
    "        return np.hstack((X, new_feature))  # Append the new feature to the dataset\n",
    "\n",
    "# 4️⃣ Define a pipeline with multiple preprocessing steps\n",
    "pipeline = Pipeline([\n",
    "    ('custom_transform', CustomFeatureTransformer()),  # Manual transformation\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Handle missing values\n",
    "    ('scaler', StandardScaler()),  # Normalize features\n",
    "    ('feature_selection', SelectKBest(f_classif, k=3)),  # Select top 3 features\n",
    "    ('model', RandomForestClassifier(n_estimators=100, random_state=42))  # Train the model\n",
    "])\n",
    "\n",
    "# 5️⃣ Train the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# 6️⃣ Save the trained pipeline\n",
    "joblib.dump(pipeline, \"iris_pipeline_custom.pkl\")\n",
    "\n",
    "# 7️⃣ Load the saved pipeline\n",
    "loaded_pipeline = joblib.load(\"iris_pipeline_custom.pkl\")\n",
    "\n",
    "# 8️⃣ Make predictions (preprocessing + model inference in one step)\n",
    "y_pred = loaded_pipeline.predict(X_test)\n",
    "print(\"Sample predictions:\", y_pred[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import onnx\n",
    "import skl2onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 1️⃣ Load dataset\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=['sepal_length', 'sepal_width', 'petal_length', 'petal_width'])\n",
    "y = iris.target\n",
    "\n",
    "# Simulate categorical features for demonstration\n",
    "X['flower_type'] = np.random.choice(['A', 'B', 'C'], size=len(X))  # Fake categorical feature\n",
    "\n",
    "# 2️⃣ Define preprocessing: Scale numerical features & encode categorical ones\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('scaler', StandardScaler(), ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'), ['flower_type'])\n",
    "])\n",
    "\n",
    "# 3️⃣ Create pipeline with preprocessing + model\n",
    "model = LogisticRegression()\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# 4️⃣ Train the pipeline\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# 5️⃣ Convert to ONNX format\n",
    "initial_type = [('float_input', FloatTensorType([None, X.shape[1] - 1 + 3]))]  # Adjust input shape\n",
    "onnx_model = convert_sklearn(pipeline, initial_types=initial_type)\n",
    "onnx.save_model(onnx_model, \"logistic_regression_pipeline.onnx\")\n",
    "\n",
    "# 6️⃣ Load ONNX model for inference\n",
    "ort_session = ort.InferenceSession(\"logistic_regression_pipeline.onnx\")\n",
    "input_name = ort_session.get_inputs()[0].name\n",
    "\n",
    "# 7️⃣ Prepare new data for inference (must match training structure)\n",
    "new_data = pd.DataFrame({\n",
    "    'sepal_length': [5.1],\n",
    "    'sepal_width': [3.5],\n",
    "    'petal_length': [1.4],\n",
    "    'petal_width': [0.2],\n",
    "    'flower_type': ['B']\n",
    "})\n",
    "\n",
    "# Apply same preprocessing before ONNX inference\n",
    "new_data_transformed = preprocessor.transform(new_data)\n",
    "\n",
    "# 8️⃣ Run inference using ONNX\n",
    "predictions = ort_session.run(None, {input_name: new_data_transformed.astype('float32')})\n",
    "\n",
    "print(\"ONNX Predictions:\", predictions[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A confidence score represents how sure a model is about its prediction. It is typically a probability value between 0 and 1.Confidence scores are particularly useful in classification models, where they provide a measure of certainty for each predicted class.\n",
    "* Artifacts are all the files generated during the ML workflow: datasets, save model, pipelines, logs, store results, etc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
