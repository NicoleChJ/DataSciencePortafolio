{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Infraestructure\n",
    "\n",
    "Once the model is packaged, it needs to be deployed using an appropriate infrastructure. Deployment involves making the model accessible for predictions in a production environment, ensuring scalability, reliability, and maintainability. Below are the key considerations and steps for deploying a machine learning model.\n",
    "\n",
    "1) Deployment pattern: \n",
    "    - Batch Processing: Best for periodic predictions (e.g., fraud detection, risk analysis), normally used scheduled jobs or pipelines that run the model periodically.\n",
    "        - Data Processing Pipelines: Using Apache Airflow, Prefect, or Kubeflow.\n",
    "        - ETL/ELT Jobs: Scheduled jobs using AWS Glue, Azure Data Factory, or custom scripts.\n",
    "    - Real-Time Inference: For applications needing instant responses (e.g., recommendation systems, chatbots). \n",
    "        - REST APIs: Using Flask, FastAPI, Django REST Framework.\n",
    "        - gRPC its more eficient but complex.Also good for web aplications\n",
    "    - Edge Deployment : Running models on IoT or mobile devices (e.g., image recognition on phones). Might use lightweight APIs, SDKs, or specialized protocols\n",
    "\n",
    "2) Environment management: how the model will run consistently across different environment.\n",
    "    - Virtual Environments: Use venv or conda to manage dependencies in different environments. \n",
    "    - Containerization\n",
    "        1) Docker: Enables portability, reproducibility and isolated environments. Packages model, dependencies, and API into a lightweight container.\n",
    "        2) Kubernetes: A container orchestration platform designed for scaling deployments in cloud environments. It efficiently manages multiple Docker containers, automating deployment, scaling, and operations across distributed infrastructure. So it can manage several model instances.\n",
    "\n",
    "\n",
    "3) Infraestructure selection: define how the model will be accessed \n",
    "    - AWS: Deploy via SageMaker, Lambda, ECS/Fargate, S3 (batch), API Gateway.\n",
    "    - Azure: Use Azure ML, Azure Functions, AKS (Kubernetes), Azure Batch.\n",
    "    - GCP: Deploy with Vertex AI, Cloud Functions, AI Platform, Cloud Run.\n",
    "    - On-Premise / Hybrid: Use Kubernetes (K8s), OpenShift, MLflow for flexible deployments.\n",
    "\n",
    "\n",
    "\n",
    "4) Automation & DevOps: ensure the model deployment process is efficient, reproducible, and scalable across various environments. This involves two key components:\n",
    "    - Infrastructure as Code (IaC): Automates the provisioning and management of infrastructure, ensuring reproducibility and scalability.Implementation varies by deployment pattern:\n",
    "        - Batch Processing: Use IaC tools like Terraform, AWS CloudFormation, or Azure Bicep to automate the setup of batch processing pipelines (AWS Batch, Azure Batch).\n",
    "        - Cloud Platforms: Use IaC tools (e.g., Terraform, CloudFormation) to automate the provisioning of cloud resources (e.g., SageMaker endpoints, Lambda functions), maintaining consistent configurations across environments.\n",
    "        - On-Premise/Hybrid: Implement IaC to manage Kubernetes clusters for containerized model deployment, ensuring consistent infrastructure across private and public clouds.\n",
    "    - CI/CD for ML Deployment:Automates the testing, building, and deployment of models using GitHub Actions, GitLab CI/CD, or Jenkins.Implement monitoring and rollback strategies for failed deployments.Normally Real-Time Inference use CI/CD pipelines for deployment of API-based models.\n",
    "\n",
    " \n",
    "\n",
    "Relationships Between Components\n",
    "\n",
    "- Deployment Pattern → Environment → Infrastructure: Your chosen deployment pattern influences your environment management needs, which in turn determines the most appropriate infrastructure.\n",
    "\n",
    "- Environment Management → API Development: Your environment management strategy affects how you develop and deploy APIs. For example, containerized environments require different API deployment approaches than virtual environments.\n",
    "\n",
    "- Infrastructure → Automation: Different infrastructure providers require different automation tools. AWS deployments might use CloudFormation, while multi-cloud deployments benefit from Terraform.\n",
    "\n",
    "\n",
    "More common Deployments:\n",
    "\n",
    "\n",
    "| Entorno  | Enfoque  |\n",
    "|:------|:------:|\n",
    "| On-Premise | Docker, Kubernetes (on-premise), Virtual Environments (venv, conda), Batch Processing, gRPC APIs, Fast API |\n",
    "| Hybrid | Kubernetes (AKS, GKE, EKS), Cloud Storage + On-Prem Compute, Edge Deployment, Microservices. |\n",
    "| Cloud\t | Serverless low latence (AWS Lambda, Azure Functions), Cloud ML Services if the model is heavy (SageMaker, Vertex AI, Azure ML), REST APIs. |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rest API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flask API\n",
    "Synchronous: Handles requests one at a time (blocking I/O).\n",
    "\n",
    "Advantages: Simpler: Good for basic APIs and small-scale applications.k Use it if you need a quick prototype or a simple API.\n",
    "\n",
    "Disadvantages: Requires manual request validation: You need to handle data validation yourself.Older framework: More widely used in legacy projects.Slower: Compared to FastAPI, especially under high loads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create a Flask API for the model \n",
    "\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the trained model\n",
    "model = joblib.load('random_forest.pkl')\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    prediction = model.predict(np.array(data[\"features\"]).reshape(1, -1))\n",
    "    return jsonify({'prediction': int(prediction[0])})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastAPI API\n",
    "Asynchronous (ASGI): Can handle multiple requests concurrently (non-blocking I/O).\n",
    "\n",
    "Advantages:Automatic validation: Uses Pydantic for input validation. Much faster: Thanks to Starlette (underlying framework).Built-in Swagger and Redoc: Automatically generates API documentation. Modern and recommended: Ideal for production-level machine learning APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load the pretained model\n",
    "with open(\"modelo_fraude.pkl\", \"rb\") as file:\n",
    "    modelo = pickle.load(file)\n",
    "\n",
    "@app.post(\"/predecir\")\n",
    "def predecir(datos: dict):\n",
    "    df = pd.DataFrame([datos])\n",
    "    prediccion = modelo.predict(df)\n",
    "    return {\"fraude\": bool(prediccion[0])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CI/CD for ML Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GitHub Actions\n",
    "Use it for: \n",
    "- Train the model\n",
    "- Packged on docker Docker.\n",
    "- Uploaded to he cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name: Deploy Model\n",
    "\n",
    "on: push\n",
    "\n",
    "jobs:\n",
    "  deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v2\n",
    "\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "\n",
    "      - name: Install dependencies\n",
    "        run: pip install -r requirements.txt\n",
    "\n",
    "      - name: Train the Model\n",
    "        run: python train.py\n",
    "\n",
    "      - name: Build Docker image\n",
    "        run: docker build -t mi-api-fraude .\n",
    "\n",
    "      - name: Push Docker image to AWS\n",
    "        run: docker push mi-repo-docker/mi-api-fraude\n",
    "\n",
    "      - name: Deploy to AWS Lambda\n",
    "        run: aws lambda update-function-code --function-name mi-api --image-uri mi-repo-docker/mi-api-fraude\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
