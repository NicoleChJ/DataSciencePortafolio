{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Types\n",
    "\n",
    "Base on your type of problem is the model that need to be used, some examples of traditional ML are:\n",
    "\n",
    "1. Supervised Learning:\n",
    "\n",
    " - Classification: Logistic Regression, Decision Trees, Random Forest, SVM,Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost),KNN, Naive Bayes,Neuronal Networks (tbd).\n",
    "\n",
    " - Regression: Linear Regression, Multiple Linear Regression,Polynomial Regression, Decision Trees, Random Forest, Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost), KNN, Support Vector Regression (SVR),Neuronal Networks (tbd)\n",
    "\n",
    "2. Unsupervised Learning: K-Means Clustering, Hierarchical Clustering\n",
    "\n",
    "3. Time Series Models: ARIMA, LSTM (Long Short-Term Memory), SARIMA (Seasonal AutoRegressive Integrated Moving Average),Prophet,  N-BEATS (tbd)\n",
    "\n",
    "\n",
    "Criteria for Model Selection:\n",
    "\n",
    "- Problem Type: Classification, regression, clustering, etc.\n",
    "- Data Size: Some models (e.g., neural networks) require large datasets, while others (e.g., decision trees) work well with smaller datasets.\n",
    "- Interpretability: Linear models are more interpretable than complex models like neural networks.\n",
    "- Computational Complexity: Some models (e.g., SVMs) are computationally expensive.\n",
    "\n",
    "\n",
    "Model Comparison: Compare multiple models to select the best one based on performance metrics or use ensemble methods that combines multiple models (e.g., bagging, boosting) to improve performance.\n",
    "\n",
    "Many machine learning libraries (e.g., XGBoost, LightGBM,tree base moldels, scikit-learn, TensorFlow) provide built-in support for parallelism, so take advantage of these options. Parallelization can consume a lot of CPU/GPU resources. Be sure to monitor your system's resource usage to avoid overloading your hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression\n",
    "\n",
    "- When to Use: \n",
    "When the relationship between features and the target is linear.\n",
    "- Advantages:\n",
    "Simple and interpretable.\n",
    "Fast to train.\n",
    "- Disadvantages:\n",
    "Assumes linearity, which may not hold in real-world data.\n",
    "Sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example data\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [1, 3, 2, 3, 5]\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X)\n",
    "print(\"MSE:\", mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Multiple Linear Regression\n",
    "- When to Use:\n",
    "When you have multiple features (independent variables) that may influence the target (dependent variable).\n",
    "When the relationship between features and the target is linear.\n",
    "- Advantages:\n",
    "Simple and interpretable.\n",
    "Fast to train and predict.\n",
    "- Disadvantages:\n",
    "Assumes linearity, which may not hold in real-world data.\n",
    "Sensitive to multicollinearity (high correlation between independent variables).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example data\n",
    "data = {\n",
    "    'X1': [1, 2, 3, 4, 5],\n",
    "    'X2': [5, 4, 3, 2, 1],\n",
    "    'y': [2, 4, 5, 4, 5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target\n",
    "X = df[['X1', 'X2']]\n",
    "y = df['y']\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Coefficients\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Coefficients:\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Regression\n",
    "\n",
    "- When to Use:\n",
    "When the relationship between the independent and dependent variables is nonlinear.\n",
    "When linear regression fails to capture the complexity of the data.\n",
    "- Advantages:\n",
    "Can model complex, nonlinear relationships.\n",
    "Flexible in capturing curvature in data.\n",
    "- Disadvantages:\n",
    "Prone to overfitting, especially with high-degree polynomials.\n",
    "Computationally expensive for high-degree polynomials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([1, 4, 9, 16, 25])\n",
    "\n",
    "# Transform features to polynomial\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_poly)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desicion Trees\n",
    "- When to Use: when interpretability is important.\n",
    "- Advantages:\n",
    "Easy to interpret and visualize.\n",
    "Handles nonlinear relationships.\n",
    "- Disadvantages:\n",
    "Prone to overfitting.\n",
    "Sensitive to small changes in data.\n",
    "\n",
    "#### Random Forest\n",
    "- When to Use: when high accuracy is required.\n",
    "- Advantages:\n",
    "Reduces overfitting compared to single decision trees.\n",
    "Handles nonlinear relationships well.\n",
    "- Disadvantages:\n",
    "Less interpretable than single decision trees.\n",
    "Slower to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1], [2], [3], [4], [5]])  # Feature\n",
    "y = np.array([1, 3, 2, 3, 5])            # Target\n",
    "\n",
    "# Train model\n",
    "model = DecisionTreeRegressor(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1], [2], [3], [4], [5]])  # Feature\n",
    "y = np.array([1, 3, 2, 3, 5])            # Target\n",
    "\n",
    "# Train model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting: XGBoost\n",
    "\n",
    "- When to Use:\n",
    "When you need high predictive accuracy.\n",
    "When computational efficiency is important (XGBoost is highly optimized).\n",
    "\n",
    "- Advantages:\n",
    "High Performance: Often outperforms other algorithms on structured data.\n",
    "Regularization: Built-in L1/L2 regularization to prevent overfitting.\n",
    "Flexibility: Supports custom loss functions and evaluation metrics.\n",
    "Handles Missing Data: Automatically handles missing values.\n",
    "\n",
    "- Disadvantages:\n",
    "Complexity: Requires careful tuning of hyperparameters.\n",
    "Computationally Expensive: Can be slow for very large datasets.\n",
    "Less Interpretable: Compared to simpler models like linear regression.\n",
    "Requires one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model for regression\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (XGBoost):\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting: LightGBM \n",
    "\n",
    "\n",
    "- When to Use:\n",
    "For large datasets with many features.\n",
    "When training speed is critical.\n",
    "\n",
    "- Advantages:\n",
    "Speed: Faster training compared to XGBoost, especially on large datasets.Is designed for efficiency and scalability\n",
    "Memory Efficiency: Uses less memory due to histogram-based splitting.\n",
    "Handles Categorical Features: Automatically handles categorical variables.\n",
    "Scalability: Works well with distributed computing.\n",
    "\n",
    "- Disadvantages:\n",
    "Overfitting: Can overfit on small datasets if not properly tuned.\n",
    "Complexity: Requires hyperparameter tuning.\n",
    "Less Robust to Noisy Data: Compared to XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train LightGBM model for regression\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (LightGBM):\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting: CatBoost\n",
    "\n",
    "- When to Use:\n",
    "When your dataset contains categorical features.\n",
    "When you want a high-performance model with minimal preprocessing.\n",
    "When you need a model that is robust to overfitting.\n",
    "\n",
    "- Advantages:\n",
    "Handles Categorical Features: Automatically encodes categorical variables, reducing the need for manual preprocessing.\n",
    "High Accuracy: Often outperforms other gradient boosting algorithms like XGBoost and LightGBM.\n",
    "Robust to Overfitting: Uses techniques like ordered boosting to reduce overfitting.\n",
    "GPU Support: Can be accelerated using GPUs for faster training (task_type parameter).\n",
    "\n",
    "- Disadvantages:\n",
    "Slower Training: Compared to LightGBM, CatBoost can be slower, especially on large datasets.\n",
    "Less Customizable: Fewer hyperparameters to tune compared to XGBoost.\n",
    "Memory Intensive: Requires more memory than some other gradient boosting algorithms.\n",
    "\n",
    "\n",
    "CatBoost automatically handles missing values, so you don’t need to impute them manually. However, you can specify how missing values are handled using the nan_mode parameter:\n",
    "* nan_mode='Min': Treat missing values as the minimum value.\n",
    "* nan_mode='Max': Treat missing values as the maximum value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset with categorical features\n",
    "data = {\n",
    "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'feature2': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],  # Categorical feature\n",
    "    'target': [10, 20, 15, 25, 30, 35, 40, 45, 50, 55]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoostRegressor\n",
    "model = CatBoostRegressor(\n",
    "    cat_features=['feature2'],  # Specify categorical features\n",
    "    iterations=100,             # Number of boosting iterations\n",
    "    learning_rate=0.1,          # Learning rate\n",
    "    verbose=0,                  # Disable logging\n",
    "    task_type='GPU'  # Enable GPU acceleration\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "\n",
    "Is a non-parametric algorithm. It predicts the target variable by finding the k most similar instances (neighbors) in the training data and averaging their values (for regression).\n",
    "\n",
    "- When to Use:\n",
    "When the data is small to medium-sized (k-NN is computationally expensive for large datasets).\n",
    "When the decision boundary is nonlinear.\n",
    "When interpretability is important (you can explain predictions based on neighbors).\n",
    "- Advantages:\n",
    "Simple and easy to implement.\n",
    "No training phase (lazy learning).\n",
    "Works well with nonlinear relationships.\n",
    "- Disadvantages:\n",
    "Computationally expensive for large datasets (requires storing the entire dataset).\n",
    "Sensitive to the choice of k and distance metric.\n",
    "Requires feature scaling (since it relies on distance calculations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
    "y = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20])  # y = 2 * X\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling (important for k-NN)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train k-NN regression model\n",
    "model = KNeighborsRegressor(n_neighbors=3)  # k=3\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Predictions:\", y_pred)\n",
    "\n",
    "# Evaluate model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regression (SVR)\n",
    "\n",
    "It works by finding a hyperplane that best fits the data while allowing a margin of error (epsilon).\n",
    "\n",
    "- When to Use:\n",
    "When the data has a nonlinear relationship.\n",
    "When you need a robust model that is less sensitive to outliers.\n",
    "- Advantages:\n",
    "Effective in high-dimensional spaces.\n",
    "Can model nonlinear relationships using kernel functions (e.g., RBF, polynomial).\n",
    "- Disadvantages:\n",
    "Computationally expensive for large datasets.\n",
    "Requires careful tuning of hyperparameters (e.g., kernel, C, epsilon).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example data\n",
    "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
    "y = np.array([1, 4, 9, 16, 25])\n",
    "\n",
    "# Train model\n",
    "model = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "- When to Use: For classification problems where the target is categorical.\n",
    "- Advantages:\n",
    "Simple and interpretable.\n",
    "Works well with small datasets.\n",
    "- Disadvantages:\n",
    "Assumes linear decision boundaries.\n",
    "Struggles with nonlinear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example data\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [0, 0, 1, 1, 1]\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X)\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desicion Trees\n",
    "- When to Use: when interpretability is important.\n",
    "- Advantages:\n",
    "Easy to interpret and visualize.\n",
    "Handles nonlinear relationships.\n",
    "- Disadvantages:\n",
    "Prone to overfitting.\n",
    "Sensitive to small changes in data.\n",
    "\n",
    "#### Random Forest\n",
    "- When to Use: when high accuracy is required.\n",
    "- Advantages:\n",
    "Reduces overfitting compared to single decision trees.\n",
    "Handles nonlinear relationships well.\n",
    "- Disadvantages:\n",
    "Less interpretable than single decision trees.\n",
    "Slower to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example data\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [0, 0, 1, 1, 1]\n",
    "\n",
    "# Train model\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X)\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example data\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [0, 0, 1, 1, 1]\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X)\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines (SVM)\n",
    "\n",
    "- When to Use: For classification tasks with clear margins of separation.\n",
    "- Advantages:\n",
    "Effective in high-dimensional spaces.\n",
    "Works well with small datasets.\n",
    "- Disadvantages:\n",
    "Computationally expensive for large datasets.\n",
    "Requires careful tuning of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example data\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [0, 0, 1, 1, 1]\n",
    "\n",
    "# Train model\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X)\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting: XGBoost\n",
    "\n",
    "- When to Use:\n",
    "When you need high predictive accuracy.\n",
    "When computational efficiency is important (XGBoost is highly optimized).\n",
    "\n",
    "- Advantages:\n",
    "High Performance: Often outperforms other algorithms on structured data.\n",
    "Regularization: Built-in L1/L2 regularization to prevent overfitting.\n",
    "Flexibility: Supports custom loss functions and evaluation metrics.\n",
    "Handles Missing Data: Automatically handles missing values.\n",
    "\n",
    "- Disadvantages:\n",
    "Complexity: Requires careful tuning of hyperparameters.\n",
    "Computationally Expensive: Can be slow for very large datasets.\n",
    "Less Interpretable: Compared to simpler models like logistic regression.\n",
    "Requires one-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example data\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "model = xgb.XGBClassifier(objective='multi:softmax', num_class=3, n_estimators=100, learning_rate=0.1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting: LightGBM\n",
    "\n",
    "- When to Use:For large datasets with many features.\n",
    "When training speed is critical.\n",
    "\n",
    "- Advantages:\n",
    "Speed: Faster training compared to XGBoost, especially on large datasets.\n",
    "Memory Efficiency: Uses less memory due to histogram-based splitting.\n",
    "Handles Categorical Features: Automatically handles categorical variables.\n",
    "Scalability: Works well with distributed computing.\n",
    "\n",
    "- Disadvantages:\n",
    "Overfitting: Can overfit on small datasets if not properly tuned.\n",
    "Complexity: Requires hyperparameter tuning.\n",
    "Less Robust to Noisy Data: Compared to XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example data\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train LightGBM model\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9\n",
    "}\n",
    "model = lgb.train(params, train_data, num_boost_round=100)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [list(x).index(max(x)) for x in y_pred]  # Convert probabilities to class labels\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boosting: CatBoost\n",
    "\n",
    "- When to Use:\n",
    "When your dataset contains categorical features.\n",
    "When you want a high-performance model with minimal preprocessing.\n",
    "When you need a model that is robust to overfitting.\n",
    "\n",
    "- Advantages:\n",
    "Handles Categorical Features: Automatically encodes categorical variables, reducing the need for manual preprocessing.\n",
    "High Accuracy: Often outperforms other gradient boosting algorithms like XGBoost and LightGBM.\n",
    "Robust to Overfitting: Uses techniques like ordered boosting to reduce overfitting.\n",
    "GPU Support: Can be accelerated using GPUs for faster training (task_type parameter)\n",
    "\n",
    "- Disadvantages:\n",
    "Slower Training: Compared to LightGBM, CatBoost can be slower, especially on large datasets.\n",
    "Less Customizable: Fewer hyperparameters to tune compared to XGBoost.\n",
    "Memory Intensive: Requires more memory than some other gradient boosting algorithms.\n",
    "\n",
    "CatBoost automatically handles missing values, so you don’t need to impute them manually. However, you can specify how missing values are handled using the nan_mode parameter:\n",
    "* nan_mode='Min': Treat missing values as the minimum value.\n",
    "* nan_mode='Max': Treat missing values as the maximum value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Example dataset with categorical features\n",
    "data = {\n",
    "    'feature1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'feature2': ['A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B'],  # Categorical feature\n",
    "    'target': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split data into features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "model = CatBoostClassifier(\n",
    "    cat_features=['feature2'],  # Specify categorical features\n",
    "    iterations=100,             # Number of boosting iterations\n",
    "    learning_rate=0.1,          # Learning rate\n",
    "    verbose=0,                   # Disable logging\n",
    "    task_type='GPU'  # Enable GPU acceleration\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN\n",
    "Is a non-parametric algorithm. It predicts the target variable by finding the k most similar instances (neighbors) in the training data and taking a majority vote (for classification).\n",
    "\n",
    "- When to Use:\n",
    "When the data is small to medium-sized (k-NN is computationally expensive for large datasets).\n",
    "When the decision boundary is nonlinear.\n",
    "When interpretability is important (you can explain predictions based on neighbors).\n",
    "- Advantages:\n",
    "Simple and easy to implement.\n",
    "No training phase (lazy learning).\n",
    "Works well with nonlinear relationships.\n",
    "- Disadvantages:\n",
    "Computationally expensive for large datasets (requires storing the entire dataset).\n",
    "Sensitive to the choice of k and distance metric.\n",
    "Requires feature scaling (since it relies on distance calculations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example data\n",
    "X = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling (important for k-NN)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train k-NN model\n",
    "model = KNeighborsClassifier(n_neighbors=3)  # k=3\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    " \n",
    " Is a probabilistic algorithm based on Bayes' Theorem. It assumes that features are independent of each other (the \"naive\" assumption), which simplifies the computation.\n",
    "\n",
    "- When to Use:\n",
    "For text classification (e.g., spam detection, sentiment analysis).\n",
    "When the dataset is small or high-dimensional.\n",
    "When you need a fast and simple model.\n",
    "- Advantages:\n",
    "Fast to train and predict.\n",
    "Works well with high-dimensional data (e.g., text data).\n",
    "Performs well even with the naive assumption of feature independence.\n",
    "- Disadvantages:\n",
    "The naive assumption of feature independence may not hold in real-world data.\n",
    "Struggles with continuous data (requires discretization).\n",
    "May produce poor results if the data distribution is not well-represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Example data\n",
    "X = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]\n",
    "y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Naive Bayes model\n",
    "model = GaussianNB()  # Gaussian Naive Bayes for continuous data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Means Clustering\n",
    "\n",
    "- When to Use: For clustering tasks where the number of clusters is known or can be estimated.\n",
    "- Advantages:\n",
    "Simple and fast.\n",
    "Works well with large datasets.\n",
    "- Disadvantages:\n",
    "Requires specifying the number of clusters.\n",
    "Sensitive to initial cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])\n",
    "\n",
    "# Train model\n",
    "model = KMeans(n_clusters=2)\n",
    "model.fit(X)\n",
    "\n",
    "# Predict clusters\n",
    "labels = model.predict(X)\n",
    "print(\"Cluster Labels:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering\n",
    "It groups similar data points into clusters by either a divisive (top-down) or agglomerative (bottom-up) approach. The result is often visualized as a dendrogram, which shows the arrangement of clusters.\n",
    "\n",
    "- When to Use:\n",
    "When you want to explore the hierarchical structure of the data.\n",
    "When the number of clusters is not known in advance.\n",
    "For small to medium-sized datasets (due to computational complexity).\n",
    "\n",
    "- Advantages:\n",
    "No Need to Specify Number of Clusters: The dendrogram helps in deciding the number of clusters.\n",
    "Interpretable: The hierarchical structure provides insights into the relationships between clusters.\n",
    "Works with Any Similarity Metric: Can use Euclidean distance, cosine similarity, etc.\n",
    "\n",
    "- Disadvantages:\n",
    "Computationally Expensive: Not suitable for large datasets (time complexity is  O(nˆ3) for agglomerative clustering).\n",
    "Sensitive to Noise and Outliers: Can produce misleading clusters if the data contains noise.\n",
    "Once a decision is made to combine clusters, it cannot be undone: This can lead to suboptimal clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate sample data\n",
    "X, _ = make_blobs(n_samples=50, centers=3, cluster_std=0.60, random_state=0)\n",
    "\n",
    "# Standardize the data (optional but recommended for clustering)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform hierarchical clustering using the \"ward\" method (minimizes variance)\n",
    "linked = linkage(X_scaled, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "\n",
    "# Cut the dendrogram to form clusters (e.g., 3 clusters)\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "clusters = fcluster(linked, t=3, criterion='maxclust')\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='viridis', s=50)\n",
    "plt.title('Hierarchical Clustering Results')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "\n",
    "\n",
    "- When to Use: For Non-spherical Clusters: When data has irregularly shaped clusters (e.g., geographical data, anomaly detection), Noise Handling: When outlier detection is important (e.g., fraud detection, environmental data) or when Variable Density Clusters: When clusters have different densities, which K-Means struggles with.\n",
    "- Advantages:\n",
    "No need to predefine clusters: Unlike K-Means, it automatically finds clusters.\n",
    "Identifies outliers: Points in low-density regions are labeled as noise.\n",
    "Works with non-linear cluster shapes : Unlike K-Means, which assumes spherical clusters.\n",
    "- Disadvantages:\n",
    "Struggles with high-dimensional data : Performance degrades when dimensions increase.\n",
    "Parameter-sensitive : The eps (neighborhood size) and min_samples need careful tuning.\n",
    "Issues with variable density clusters:  If clusters have large density variations, DBSCAN may not work well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Create sample data (2D points)\n",
    "X = np.array([[1, 2], [2, 3], [2, 2], [8, 8], [8, 9], [25, 80]])  # The last point is an outlier, anomaly detection\n",
    "\n",
    "# Apply DBSCAN\n",
    "db = DBSCAN(eps=2, min_samples=2).fit(X)\n",
    "labels = db.labels_\n",
    "\n",
    "print(\"Cluster labels:\", labels)  # -1 represents noise (outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARIMA (AutoRegressive Integrated Moving Average)\n",
    "\n",
    "- When to Use: For stationary time series data.\n",
    "- Advantages:\n",
    "Handles trends and seasonality.\n",
    "Widely used in forecasting.\n",
    "- Disadvantages:\n",
    "Requires stationary data.\n",
    "Complex to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Example data\n",
    "data = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "# Train model\n",
    "model = ARIMA(data, order=(1, 1, 1))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Forecast\n",
    "forecast = model_fit.forecast(steps=3)\n",
    "print(\"Forecast:\", forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector AutoRegression (VAR)\n",
    "VAR is an extension of ARIMA for multivariate time series forecasting. Unlike ARIMA, which models a single time series, VAR captures relationships between multiple time-dependent variables.\n",
    "\n",
    "- When to Use: Multivariate Time Series: When two or more time-dependent variables influence each other (e.g., stock prices and interest rates). Interdependencies Between Variables: When lagged values of multiple variables affect future values (e.g., economic forecasting).\n",
    "- Advantages:\n",
    "Models Multiple Time Series Together \n",
    "Well-established & interpretable\n",
    "Handles Lag Dependencies – Captures effects over time.\n",
    "- Disadvantages:\n",
    "Requires stationary data.\n",
    "High Complexity with Many Variables – Large datasets require careful selection of lag parameters.\n",
    "Assumes Linear Relationships – Non-linear dependencies may not be captured well.\n",
    "\n",
    "\n",
    "It learns relationships between all the variables and their past values (lags).The model then forecasts the next two time steps for each feature by considering:\n",
    "The past two time steps (maxlags=2). The interactions between all time series in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Sample multivariate time series data\n",
    "data = {\n",
    "    'GDP': [2.3, 2.5, 2.8, 3.0, 3.2, 3.3],  # GDP growth rate\n",
    "    'Unemployment': [5.2, 5.0, 4.8, 4.6, 4.5, 4.3]  # Unemployment rate\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit VAR model\n",
    "model = VAR(df)\n",
    "result = model.fit(maxlags=2)  # Selecting 2 lags\n",
    "\n",
    "# Forecast next 2 periods\n",
    "forecast = result.forecast(df.values[-2:], steps=2)  #columns correspond to the order of input features\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VARIMA (Vector ARIMA)\n",
    "VARIMA is a combination of VAR and ARIMA, extending ARIMA to multiple time series. It allows for both differencing (I) and moving average (MA) components in addition to autoregression (AR).\n",
    "\n",
    "- When to Use:\n",
    "Multivariate Time Series with Differencing Needed: If data has trends or seasonality that require differencing.\n",
    "Better Handling of Non-Stationarity: If VAR alone fails due to non-stationarity.\n",
    "Time Series with Moving Average Components: If errors have serial correlation.\n",
    "- Advantages:\n",
    "Captures Complex Dependencies and its more generalized than VAR.\n",
    "Handles Non-Stationary Data\n",
    "- Disadvantages:\n",
    "Computationally Expensive.\n",
    "Difficult to Interpret.\n",
    "Needs Large Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame with two time series\n",
    "data = {\n",
    "    'GDP': [2.3, 2.5, 2.8, 3.0, 3.2, 3.3],\n",
    "    'Unemployment': [5.2, 5.0, 4.8, 4.6, 4.5, 4.3]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Automatically find best VARIMA order\n",
    "model = auto_arima(df, seasonal=False, error_action='ignore', trace=True)\n",
    "\n",
    "# Fit the best model\n",
    "model.fit(df)\n",
    "\n",
    "# Forecast next 2 periods\n",
    "forecast = model.predict(n_periods=2) #columns results correspond to the order of input features\n",
    "print(forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM (Long Short-Term Memory)\n",
    "\n",
    "- When to Use: For complex time series or sequential data with long-term dependencies.\n",
    "- Advantages:\n",
    "Handles long-term dependencies.\n",
    "Works well with large datasets.\n",
    "- Disadvantages:\n",
    "Computationally expensive.\n",
    "Requires large amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "# Example data\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([2, 3, 4, 5, 6])\n",
    "\n",
    "# Reshape data for LSTM\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(1, 1)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X)\n",
    "print(\"Predictions:\", y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARIMA (Seasonal AutoRegressive Integrated Moving Average)\n",
    "Is an extension of ARIMA that explicitly models seasonality in time series data. It is useful for time series that exhibit seasonal patterns (e.g., monthly sales data with yearly seasonality).\n",
    "\n",
    "- When to Use:\n",
    "WFor forecasting tasks where seasonality is a key factor (e.g., monthly, quarterly, or yearly cycles).\n",
    "When the data is non-stationary and requires differencing to remove trends and seasonality.\n",
    "\n",
    "- Advantages:\n",
    "Explicitly models seasonality, making it suitable for seasonal time series.\n",
    "Flexible in handling both trend and seasonality.\n",
    "Widely used in forecasting applications.\n",
    "\n",
    "- Disadvantages:\n",
    "Requires careful tuning of hyperparameters (e.g., seasonal order).\n",
    "Computationally expensive for large datasets.\n",
    "Assumes that seasonality is constant over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data: Monthly sales with seasonality\n",
    "data = [100, 120, 130, 150, 200, 250, 300, 350, 400, 450, 500, 550,\n",
    "        110, 130, 140, 160, 210, 260, 310, 360, 410, 460, 510, 560]\n",
    "dates = pd.date_range(start='2020-01-01', periods=len(data), freq='M')\n",
    "df = pd.DataFrame({'date': dates, 'sales': data})\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "# Fit SARIMA model\n",
    "# SARIMA(p, d, q)(P, D, Q, S)\n",
    "# p: AR order, d: differencing, q: MA order\n",
    "# P: Seasonal AR order, D: Seasonal differencing, Q: Seasonal MA order, S: Seasonality length (e.g., 12 for monthly data)\n",
    "model = SARIMAX(df['sales'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\n",
    "results = model.fit(disp=False)\n",
    "\n",
    "# Forecast the next 12 months\n",
    "forecast = results.get_forecast(steps=12)\n",
    "forecast_mean = forecast.predicted_mean\n",
    "confidence_intervals = forecast.conf_int()\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df.index, df['sales'], label='Observed')\n",
    "plt.plot(forecast_mean.index, forecast_mean, label='Forecast', color='red')\n",
    "plt.fill_between(confidence_intervals.index,\n",
    "                 confidence_intervals.iloc[:, 0],\n",
    "                 confidence_intervals.iloc[:, 1], color='pink', alpha=0.3)\n",
    "plt.title('SARIMA Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prophet\n",
    "\n",
    "- When to Use:\n",
    "When the time series has strong seasonality (e.g., daily, weekly, or yearly patterns).\n",
    "When you want to include holiday effects or other known events.\n",
    "For quick and robust forecasting without extensive tuning.\n",
    "\n",
    "- Advantages:\n",
    "Easy to use and requires minimal hyperparameter tuning.\n",
    "Handles missing data and outliers gracefully.\n",
    "Provides intuitive forecasts with uncertainty intervals.\n",
    "\n",
    "- Disadvantages:\n",
    "Less flexible than SARIMA for custom seasonality or trend modeling.\n",
    "May not perform well on time series without clear seasonality.\n",
    "Requires additional setup for custom seasonality or holiday effects.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data: Daily sales with seasonality\n",
    "data = {\n",
    "    'ds': pd.date_range(start='2020-01-01', periods=365, freq='D'),\n",
    "    'y': [100 + 10 * (i % 30) + 5 * (i % 7) for i in range(365)]  # Simulated seasonal data\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize and fit Prophet model\n",
    "model = Prophet()\n",
    "model.fit(df)\n",
    "\n",
    "# Create future dataframe for forecasting\n",
    "future = model.make_future_dataframe(periods=30)  # Forecast the next 30 days\n",
    "\n",
    "# Make predictions\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Plot the forecast\n",
    "fig = model.plot(forecast)\n",
    "plt.title('Prophet Forecast')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.show()\n",
    "\n",
    "# Plot components (trend, weekly seasonality, yearly seasonality)\n",
    "fig2 = model.plot_components(forecast)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
