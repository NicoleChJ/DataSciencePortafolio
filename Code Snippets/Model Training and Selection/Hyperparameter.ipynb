{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tunning\n",
    "Can be a computationally expensive task, but it can be parallelized to speed up the process. Parallelization can consume a lot of CPU/GPU resources. Be sure to monitor your system's resource usage to avoid overloading your hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "Is a systematic approach where all possible combinations of hyperparameters are evaluated. \n",
    "\n",
    "- When to Use:\n",
    "When the hyperparameter space is small and computationally feasible to explore exhaustively.\n",
    "When you want to ensure that you find the best possible combination of hyperparameters.\n",
    "- Advantages:\n",
    "Exhaustive: Guarantees finding the best combination within the specified grid.\n",
    "Easy to implement and understand.\n",
    "- Disadvantages:\n",
    "Computationally expensive, especially with a large number of hyperparameters or a wide range of values.\n",
    "Inefficient for high-dimensional hyperparameter spaces.\n",
    "Suffers from \"curse of dimensionality\".\n",
    "Inefficient when some parameters are more important than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy') # n_jobs=-1,use all available cores to run cross-validation on the grid of hyperparameters.\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define models and parameter grids\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 1],\n",
    "            'max_depth': [3, 5, 10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform Grid Search for each model\n",
    "best_models = {}\n",
    "for model_name, model_info in models.items():\n",
    "    grid_search = GridSearchCV(model_info['model'], model_info['params'], cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    best_models[model_name] = {\n",
    "        'best_model': grid_search.best_estimator_,\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_\n",
    "    }\n",
    "\n",
    "# Select the best model\n",
    "best_model_name = max(best_models, key=lambda x: best_models[x]['best_score'])\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best Parameters: {best_models[best_model_name]['best_params']}\")\n",
    "print(f\"Best Score: {best_models[best_model_name]['best_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "Randomly samples hyperparameters from a specified distribution or range.\n",
    "\n",
    "- When to Use:\n",
    "When the hyperparameter space is large, and Grid Search is computationally infeasible.\n",
    "When you want to explore a wide range of hyperparameters efficiently.\n",
    "- Advantages:\n",
    "More efficient than Grid Search for large hyperparameter spaces.\n",
    "Can find good hyperparameters with fewer iterations.\n",
    "- Disadvantages:\n",
    "Does not guarantee finding the best combination of hyperparameters.\n",
    "May miss optimal hyperparameters if the search space is not well-defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter distribution\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': randint(2, 11)\n",
    "}\n",
    "\n",
    "# Perform Random Search\n",
    "random_search = RandomizedSearchCV(model, param_dist, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "print(\"Best Score:\", random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define models and parameter distributions\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': randint(50, 200),\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': randint(2, 11)\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'C': uniform(0.1, 10),\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': randint(50, 200),\n",
    "            'learning_rate': uniform(0.01, 1),\n",
    "            'max_depth': randint(3, 10)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform Random Search for each model\n",
    "best_models = {}\n",
    "for model_name, model_info in models.items():\n",
    "    random_search = RandomizedSearchCV(model_info['model'], model_info['params'], n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "    random_search.fit(X, y)\n",
    "    best_models[model_name] = {\n",
    "        'best_model': random_search.best_estimator_,\n",
    "        'best_score': random_search.best_score_,\n",
    "        'best_params': random_search.best_params_\n",
    "    }\n",
    "\n",
    "# Select the best model\n",
    "best_model_name = max(best_models, key=lambda x: best_models[x]['best_score'])\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best Parameters: {best_models[best_model_name]['best_params']}\")\n",
    "print(f\"Best Score: {best_models[best_model_name]['best_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Bayesian Optimization\n",
    "Bayesian Optimization uses probabilistic models (e.g., Gaussian Processes) to find the optimal hyperparameters by modeling the performance of the model as a function of the hyperparameters.\n",
    "\n",
    "- When to Use:\n",
    "When the hyperparameter space is large, and you want to find the optimal hyperparameters with fewer evaluations.\n",
    "When the objective function (e.g., model performance) is expensive to evaluate.\n",
    "- Advantages:\n",
    "Efficient: Requires fewer evaluations compared to Grid Search and Random Search.\n",
    "Balances exploration and exploitation.\n",
    "- Disadvantages:\n",
    "More complex to implement and understand.Might get stuck in local optima\n",
    "Requires a good probabilistic model to guide the search.\n",
    "Less parallelizable than Grid/Random Search\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_space = {\n",
    "    'n_estimators': (50, 200),\n",
    "    'max_depth': (1, 20),\n",
    "    'min_samples_split': (2, 10)\n",
    "}\n",
    "\n",
    "# Perform Bayesian Optimization\n",
    "bayes_search = BayesSearchCV(model, param_space, n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "bayes_search.fit(X, y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", bayes_search.best_params_)\n",
    "print(\"Best Score:\", bayes_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt import BayesSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Define models and parameter search spaces\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': (50, 200),\n",
    "            'max_depth': (1, 20),\n",
    "            'min_samples_split': (2, 10)\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'C': (0.1, 10),\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': (50, 200),\n",
    "            'learning_rate': (0.01, 1),\n",
    "            'max_depth': (3, 10)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform Bayesian Optimization for each model\n",
    "best_models = {}\n",
    "for model_name, model_info in models.items():\n",
    "    bayes_search = BayesSearchCV(model_info['model'], model_info['params'], n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
    "    bayes_search.fit(X, y)\n",
    "    best_models[model_name] = {\n",
    "        'best_model': bayes_search.best_estimator_,\n",
    "        'best_score': bayes_search.best_score_,\n",
    "        'best_params': bayes_search.best_params_\n",
    "    }\n",
    "\n",
    "# Select the best model\n",
    "best_model_name = max(best_models, key=lambda x: best_models[x]['best_score'])\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best Parameters: {best_models[best_model_name]['best_params']}\")\n",
    "print(f\"Best Score: {best_models[best_model_name]['best_score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Hyperband\n",
    "Bandit-based optimization technique that speeds up hyperparameter tuning by early stopping poorly performing configurations.\n",
    "\n",
    "- When to Use:\n",
    "When you have a large hyperparameter space and want to quickly eliminate poor configurations.\n",
    "When the model training process is time-consuming.\n",
    "- Advantages:\n",
    "Efficient: Reduces the number of evaluations by early stopping.\n",
    "Works well with large hyperparameter spaces.\n",
    "- Disadvantages:\n",
    "Requires the model to support early stopping.\n",
    "More complex to implement compared to Grid Search and Random Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hyperband import HyperbandSearchCV  # Requires external library\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter search space\n",
    "param_space = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Perform Hyperband Search\n",
    "hyperband_search = HyperbandSearchCV(model, param_space, max_iter=81, cv=5, scoring='accuracy', random_state=42)\n",
    "hyperband_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", hyperband_search.best_params_)\n",
    "print(\"Best Score:\", hyperband_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred = hyperband_search.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from hyperband import HyperbandSearchCV  # Requires external library\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models and parameter search spaces\n",
    "models = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 10, 20],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 1],\n",
    "            'max_depth': [3, 5, 10]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Perform Hyperband Search for each model\n",
    "best_models = {}\n",
    "for model_name, model_info in models.items():\n",
    "    hyperband_search = HyperbandSearchCV(model_info['model'], model_info['params'], max_iter=81, cv=5, scoring='accuracy', random_state=42)\n",
    "    hyperband_search.fit(X_train, y_train)\n",
    "    best_models[model_name] = {\n",
    "        'best_model': hyperband_search.best_estimator_,\n",
    "        'best_score': hyperband_search.best_score_,\n",
    "        'best_params': hyperband_search.best_params_\n",
    "    }\n",
    "\n",
    "# Select the best model\n",
    "best_model_name = max(best_models, key=lambda x: best_models[x]['best_score'])\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Best Parameters: {best_models[best_model_name]['best_params']}\")\n",
    "print(f\"Best Score: {best_models[best_model_name]['best_score']}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_model = best_models[best_model_name]['best_model']\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
