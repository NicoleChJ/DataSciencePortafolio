{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics\n",
    "\n",
    "Essential for evaluating the effectiveness of machine learning models. The choice of metric depends on the type of problem (classification, regression, clustering, or time series) and the specific goals of the analysis. Below, we break down the most commonly used metrics for each type of problem.\n",
    "\n",
    "Learning Curves: Plot training and validation performance over time to diagnose overfitting or underfitting.\n",
    "\n",
    "## Classification Metrics\n",
    "- Confusion Matrix: For classification problems, use a confusion matrix to visualize true positives, false positives, etc.\n",
    "- Accuracy: Proportion of correctly classified instances. When to use: Balanced classes, equal misclassification costs\n",
    "- Precision/Recall: Focus on positive class performance When to use: Imbalanced classes, asymmetric costs (e.g., fraud detection)\n",
    "- F1-Score: Harmonic mean of precision and recall. When to use: Need balance between precision and recall\n",
    "- ROC-AUC: Area under ROC curve, measures discrimination.When to use: Ranking quality, threshold-independent evaluation\n",
    "- PR-AUC: Area under precision-recall curve.When to use: Highly imbalanced datasets\n",
    "\n",
    "## Regression Metrics\n",
    "- Mean Squared Error (MSE): Average squared difference between predictions and actuals.When to use: General purpose, penalizes large errors\n",
    "- Mean Absolute Error (MAE): Average absolute difference.When to use: Need for interpretability, less sensitivity to outliers\n",
    "- R-squared: Proportion of variance explained.When to use: Comparing models, explaining fit to stakeholders\n",
    "- MAPE: Mean absolute percentage error.When to use: Comparing errors across different scales\n",
    "\n",
    "## Clustering: \n",
    "- Silhouette score\n",
    "- Davies-Bouldin index.\n",
    "\n",
    "## Time Series: \n",
    "- Mean Absolute Percentage Error (MAPE)\n",
    "- AIC\n",
    "- BIC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "\n",
    "Plot training and validation performance (e.g., accuracy, loss) over time (epochs or iterations) to diagnose overfitting or underfitting.\n",
    "\n",
    "- When to Use:\n",
    "During model training to monitor performance.\n",
    "To determine if the model needs more data, regularization, or architectural changes.\n",
    "- Interpretation:\n",
    "Overfitting: Training performance improves, but validation performance plateaus or worsens.\n",
    "Underfitting: Both training and validation performance are poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "A table that visualizes the performance of a classification model by showing true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
