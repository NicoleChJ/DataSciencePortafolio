{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced data occurs when the classes in a classification problem are not evenly distributed. This can lead to a biased model that favors the majority class.\n",
    "Example: In a fraud detection dataset, 98% of transactions may be non-fraudulent, while only 2% are fraudulent. A naive model could predict \"non-fraud\" for every case and still be 98% accurate, but it wouldn’t detect fraud effectively.\n",
    "\n",
    "Remember the test data must remain representative of the actual distribution of classes in the real-world scenario. You want to evaluate your model's performance on data that is as close to the real distribution as possible, and applying resampling techniques to the test data would distort that representation. \n",
    "\n",
    "### Resampling Methods\n",
    "- Oversampling (SMOTE, ADASYN, Random Oversampling).Use when: The dataset is small, and you need more instances of the minority class.How it works: Duplicates or synthetically generates new examples for the minority class.\n",
    "\n",
    "- Undersampling (Random Undersampling, Tomek Links, Edited Nearest Neighbors). Use when: You have a very large dataset, and keeping all majority class examples is unnecessary.How it works: Removes instances from the majority class to balance the dataset.\n",
    "\n",
    "### Algorithmic Techniques\n",
    "- Class Weighting(Several ML models RF, SVM, KNN,etc) Use when: The dataset is imbalanced, but you don’t want to modify the data.How it works: Assigns a higher weight to the minority class, making misclassifications costlier.\n",
    "\n",
    "- Anomaly Detection Methods (One-Class SVM, Isolation Forest).Use when: The minority class represents anomalies (e.g., fraud detection).How it works: Models learn normal behavior and flag deviations as anomalies.\n",
    "\n",
    "Evaluation Metrics for Imbalanced Data. Using accuracy alone is misleading, instead, use:\n",
    "- Precision, Recall, and F1-score\n",
    "- ROC-AUC and PR-AUC (Precision-Recall Curve)\n",
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Oversampling (Increasing Minority Class):Randomly duplicates instances of the minority class to balance the dataset.\n",
    "#Best for small datasets where adding exact duplicates is acceptable.\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "\n",
    "# Apply Random Oversampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution after Random Oversampling:\", np.bincount(y_resampled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOTE (Synthetic Minority Oversampling Technique)\n",
    "#Generates synthetic data points based on the nearest neighbors of the minority class.\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution after SMOTE:\", np.bincount(y_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADASYN (Adaptive Synthetic Sampling)\n",
    "#imilar to SMOTE but generates more synthetic samples for harder-to-learn cases.\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# Apply ADASYN\n",
    "adasyn = ADASYN(random_state=42)\n",
    "X_resampled, y_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution after ADASYN:\", np.bincount(y_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Undersampling (Reducing Majority Class)\n",
    "#Randomly removes instances of the majority class to balance the dataset.\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Apply Random Undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution after Random Undersampling:\", np.bincount(y_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tomek Links (Undersampling by Removing Closely Related Pairs)\n",
    "#Removes majority class samples that are closest to the minority class. Best for reducing noise and making class separation clearer.\n",
    "\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "# Apply Tomek Links\n",
    "tomek = TomekLinks()\n",
    "X_resampled, y_resampled = tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution after Tomek Links:\", np.bincount(y_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENN works by analyzing each sample in the majority class and removes majority class samples that are surrounded by minority class samples.\n",
    "#When to Use: Use ENN when you have noisy samples in the majority class that could confuse the learning process.\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split into training and testing datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply Edited Nearest Neighbors (ENN)\n",
    "enn = EditedNearestNeighbours()\n",
    "X_resampled, y_resampled = enn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check the class distribution after ENN\n",
    "print(\"Class distribution after ENN:\", np.bincount(y_resampled))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + Tomek Links (Hybrid Approach)\n",
    "#Combines SMOTE (oversampling) with Tomek Links (undersampling).Best for reducing noise and ensuring a well-balanced dataset.\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Apply SMOTE + Tomek Links\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"Class distribution after SMOTE + Tomek Links:\", np.bincount(y_resampled))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
