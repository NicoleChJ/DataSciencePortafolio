{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics\n",
    "\n",
    "Essential for evaluating the effectiveness of machine learning models. The choice of metric depends on the type of problem (classification, regression, clustering, or time series) and the specific goals of the analysis. Below, we break down the most commonly used metrics for each type of problem.\n",
    "\n",
    "- Learning Curves: Plot training and validation performance over time to diagnose overfitting or underfitting.\n",
    "\n",
    "## Classification Metrics\n",
    "- Confusion Matrix: For classification problems, use a confusion matrix to visualize true positives, false positives, etc.\n",
    "- Accuracy: Proportion of correctly classified instances. When to use: Balanced classes, equal misclassification costs\n",
    "- Precision: Focus on positive class performance When to use: Imbalanced classes, asymmetric costs (e.g., fraud detection)\n",
    "- Recall: Measures the proportion of true positives among all actual positives.When the cost of false negatives is high (e.g., disease detection).\n",
    "- F1-Score: Harmonic mean of precision and recall. When to use: Need balance between precision and recall\n",
    "- ROC-AUC: Area under ROC curve, measures discrimination.When to use: Ranking quality, threshold-independent evaluation\n",
    "- PR-AUC: Area under precision-recall curve.When to use: Highly imbalanced datasets\n",
    "\n",
    "## Regression Metrics\n",
    "- Mean Squared Error (MSE): Average squared difference between predictions and actuals.When to use: General purpose, penalizes large errors\n",
    "- Mean Absolute Error (MAE): Average absolute difference.When to use: Need for interpretability, less sensitivity to outliers\n",
    "- R-squared: Proportion of variance explained.When to use: Comparing models, explaining fit to stakeholders\n",
    "- MAPE: Mean absolute percentage error.When to use: Comparing errors across different scales\n",
    "\n",
    "## Clustering: \n",
    "- Silhouette score: Measures how similar an object is to its own cluster compared to other clusters.\n",
    "- Davies-Bouldin index: Measures the average similarity ratio of each cluster with the cluster that is most similar to it.\n",
    "\n",
    "## Time Series: \n",
    "- Mean Absolute Percentage Error (MAPE):  Mean absolute percentage error for time series forecasting.\n",
    "- AIC (Akaike Information Criterion): Measures the trade-off between model complexity and goodness of fit.\n",
    "- BIC (Bayesian Information Criterion): Similar to AIC but penalizes model complexity more heavily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves\n",
    "\n",
    "Can be used for classification, regression, time series, and clustering problems. They are a versatile diagnostic tool to understand how a model is performing during training and to identify potential issues like overfitting or underfitting\n",
    "\n",
    "- When to Use:\n",
    "During model training to monitor performance.\n",
    "To determine if the model needs more data, regularization, or architectural changes.\n",
    "- Interpretation:\n",
    "Overfitting: Training performance improves, but validation performance plateaus or worsens\n",
    "Underfitting: Both training and validation performance are poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curves for Classification\n",
    "To monitor the performance of a classification model (e.g., accuracy, precision, recall, F1-score) on the training set and validation set over time (epochs or iterations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Generate learning curves\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    RandomForestClassifier(), X, y, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "# Plot learning curves\n",
    "plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Score')\n",
    "plt.plot(train_sizes, np.mean(val_scores, axis=1), label='Validation Score')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curves (Classification)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curves for Regression\n",
    "To monitor the performance of a regression model (e.g., MSE, MAE, R²) on the training set and validation set over time.\n",
    "\n",
    "- When to use: \n",
    "To diagnose overfitting or underfitting in regression tasks.\n",
    "To determine if the model needs more data or regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1)\n",
    "\n",
    "# Generate learning curves\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    LinearRegression(), X, y, cv=5, scoring='neg_mean_squared_error', train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "# Plot learning curves\n",
    "plt.plot(train_sizes, -np.mean(train_scores, axis=1), label='Training MSE')\n",
    "plt.plot(train_sizes, -np.mean(val_scores, axis=1), label='Validation MSE')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Learning Curves (Regression)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curves for Time Series\n",
    "To monitor the performance of a time series model (e.g., MAPE, RMSE) on the training set and validation set over time.\n",
    "- When to use: To diagnose overfitting or underfitting in time series forecasting tasks.\n",
    "To determine if the model needs more data or better feature engineering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic time series data\n",
    "X = np.array([i for i in range(100)])\n",
    "y = np.sin(X) + np.random.normal(0, 0.1, 100)\n",
    "\n",
    "# Time Series Split\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Generate learning curves\n",
    "train_sizes = []\n",
    "train_scores = []\n",
    "val_scores = []\n",
    "for train_index, val_index in tscv.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_train.reshape(-1, 1), y_train)\n",
    "    train_scores.append(model.score(X_train.reshape(-1, 1), y_train))\n",
    "    val_scores.append(model.score(X_val.reshape(-1, 1), y_val))\n",
    "    train_sizes.append(len(X_train))\n",
    "\n",
    "# Plot learning curves\n",
    "plt.plot(train_sizes, train_scores, label='Training Score')\n",
    "plt.plot(train_sizes, val_scores, label='Validation Score')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Learning Curves (Time Series)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curves for Clustering\n",
    "\n",
    "To monitor the performance of a clustering model (e.g., silhouette score, Davies-Bouldin index) on the training set over time.For clustering, learning curves are less common but can still be used to monitor convergence or clustering quality\n",
    "- When to use:\n",
    "To diagnose if the clustering algorithm is converging or if it needs more iterations.\n",
    "To determine the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Generate synthetic clustering dataset\n",
    "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=1.0)\n",
    "\n",
    "# Generate learning curves\n",
    "train_sizes = np.linspace(0.1, 1.0, 10)\n",
    "silhouette_scores = []\n",
    "for size in train_sizes:\n",
    "    n_samples = int(size * len(X))\n",
    "    X_subset = X[:n_samples]\n",
    "    model = KMeans(n_clusters=4)\n",
    "    model.fit(X_subset)\n",
    "    silhouette_scores.append(silhouette_score(X_subset, model.labels_))\n",
    "\n",
    "# Plot learning curves\n",
    "plt.plot(train_sizes * len(X), silhouette_scores)\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Learning Curves (Clustering)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "\n",
    "A table that visualizes the performance of a classification model by showing true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=data.target_names)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy\n",
    "Measures the proportion of correctly classified instances.\n",
    "\n",
    "- When to use: When classes are balanced.\n",
    "When the cost of misclassification is symmetric (e.g., equal cost for false positives and false negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision\n",
    "Measures the proportion of true positives among all predicted positives.\n",
    "- When to use:\n",
    "When the cost of false positives is high (e.g., spam detection).\n",
    "For imbalanced datasets where the positive class is rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Calculate precision (for binary classification, use average='binary')\n",
    "precision = precision_score(y_test, y_pred, average='macro')  # For multiclass\n",
    "print(\"Precision:\", precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall / Sensitivity\n",
    "Measures the proportion of true positives among all actual positives.\n",
    "- When to Use:\n",
    "When the cost of false negatives is high (e.g., disease detection).\n",
    "For imbalanced datasets where missing positive instances is costly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Calculate recall (for binary classification, use average='binary')\n",
    "recall = recall_score(y_test, y_pred, average='macro')  # For multiclass\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-Score\n",
    "Harmonic mean of precision and recall, providing a balance between the two.\n",
    "\n",
    "-When to Use:\n",
    "When you need a single metric that balances precision and recall.\n",
    "For imbalanced datasets where both false positives and false negatives are important.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1-Score (for binary classification, use average='binary')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')  # For multiclass\n",
    "print(\"F1-Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC-AUC (Receiver Operating Characteristic - Area Under Curve)\n",
    "Measures the model's ability to discriminate between classes by plotting the true positive rate (TPR) against the false positive rate (FPR) at various thresholds.\n",
    "- When to Use:\n",
    "For threshold-independent evaluation.\n",
    "When ranking or probability outputs are important (e.g., credit scoring).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# ROC-AUC for binary classification\n",
    "# For multiclass, use `multi_class='ovr'` or `multi_class='ovo'`\n",
    "roc_auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class='ovr')\n",
    "print(\"ROC-AUC:\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PR-AUC (Precision-Recall - Area Under Curve)\n",
    "Measures the trade-off between precision and recall, especially useful for imbalanced datasets.\n",
    "\n",
    "- When to Use:\n",
    "For highly imbalanced datasets where the positive class is rare.\n",
    "When false positives and false negatives have asymmetric costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "# Precision-Recall Curve and AUC\n",
    "precision, recall, _ = precision_recall_curve(y_test, model.predict_proba(X_test)[:, 1])  # Binary classification\n",
    "pr_auc = auc(recall, precision)\n",
    "print(\"PR-AUC:\", pr_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Squared Error (MSE)\n",
    "Measures the average squared difference between predicted and actual values.\n",
    "\n",
    "- When to Use:\n",
    "For general-purpose regression evaluation.\n",
    "When large errors should be penalized more heavily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate MSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "Measures the average absolute difference between predicted and actual values.\n",
    "\n",
    "- When to Use:\n",
    "When interpretability is important.\n",
    "When the dataset contains outliers (less sensitive to outliers than MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Calculate MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R-squared (Coefficient of Determination)\n",
    "Measures the proportion of variance in the dependent variable that is explained by the model.\n",
    "- When to Use:\n",
    "For comparing models.\n",
    "When explaining model performance to stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Percentage Error (MAPE)\n",
    "Measures the average percentage error between predicted and actual values.\n",
    "\n",
    "- When to Use:\n",
    "When comparing errors across datasets with different scales.\n",
    "For business contexts where percentage errors are more intuitiv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "print(\"MAPE:\", mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Silhouette Score\n",
    "Measures how similar an object is to its own cluster compared to other clusters. Ranges from -1 (poor clustering) to 1 (excellent clustering).\n",
    "\n",
    "- When to Use:\n",
    "For evaluating the quality of clustering when ground truth labels are unavailable.\n",
    "To determine the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Generate synthetic clustering dataset\n",
    "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=1.0)\n",
    "\n",
    "# Train KMeans\n",
    "model = KMeans(n_clusters=4)\n",
    "model.fit(X)\n",
    "\n",
    "# Calculate Silhouette Score\n",
    "silhouette = silhouette_score(X, model.labels_)\n",
    "print(\"Silhouette Score:\", silhouette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Davies-Bouldin Index\n",
    "Measures the average similarity ratio of each cluster with the cluster that is most similar to it. Lower values indicate better clustering.\n",
    "- When to Use:\n",
    "For comparing clustering algorithms or configurations.\n",
    "When ground truth labels are unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "# Calculate Davies-Bouldin Index\n",
    "db_index = davies_bouldin_score(X, model.labels_)\n",
    "print(\"Davies-Bouldin Index:\", db_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Percentage Error (MAPE)\n",
    "Measures the average percentage error between predicted and actual values in time series forecasting.\n",
    "\n",
    "- When to Use:\n",
    "For evaluating time series models where percentage errors are meaningful.\n",
    "When comparing models across different time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example time series data\n",
    "y_true = np.array([100, 200, 300, 400, 500])\n",
    "y_pred = np.array([110, 190, 310, 420, 480])\n",
    "\n",
    "# Calculate MAPE\n",
    "mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "print(\"MAPE:\", mape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AIC (Akaike Information Criterion)\n",
    "Measures the trade-off between model complexity and goodness of fit. Lower values indicate better models.\n",
    "\n",
    "- When to Use:\n",
    "For comparing time series models (e.g., ARIMA, SARIMA).\n",
    "When balancing model fit and complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Example time series data\n",
    "data = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
    "\n",
    "# Fit ARIMA model\n",
    "model = ARIMA(data, order=(1, 1, 1))\n",
    "model_fit = model.fit()\n",
    "\n",
    "# Calculate AIC\n",
    "aic = model_fit.aic\n",
    "print(\"AIC:\", aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BIC (Bayesian Information Criterion)\n",
    "Similar to AIC but penalizes model complexity more heavily. Lower values indicate better models.\n",
    "- When to Use:\n",
    "For comparing time series models when simplicity is a priority.\n",
    "When selecting models for forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate BIC (using the same ARIMA model as above)\n",
    "bic = model_fit.bic\n",
    "print(\"BIC:\", bic)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
