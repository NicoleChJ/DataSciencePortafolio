{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison\n",
    "\n",
    "Evaluates multiple models to determine which one performs best on a given task. For this:\n",
    "\n",
    "1) Training multiple models on the same dataset. Start with default hyperparameters for all models, identify the top-performing models, and then tune only those.\n",
    "2) Evaluating their performance using appropriate metrics.\n",
    "3) Comparing the results to select the best model.\n",
    "\n",
    "Some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'SVM': SVC()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    results[name] = accuracy\n",
    "\n",
    "# Compare results\n",
    "for name, accuracy in results.items():\n",
    "    print(f\"{name}: Accuracy = {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate synthetic regression dataset\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'LinearRegression': LinearRegression(),\n",
    "    'RandomForest': RandomForestRegressor(),\n",
    "    'GradientBoosting': GradientBoostingRegressor()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    results[name] = mse\n",
    "\n",
    "# Compare results\n",
    "for name, mse in results.items():\n",
    "    print(f\"{name}: MSE = {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Clustering Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Generate synthetic clustering dataset\n",
    "X, _ = make_blobs(n_samples=500, centers=4, cluster_std=1.0)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'KMeans': KMeans(n_clusters=4),\n",
    "    'AgglomerativeClustering': AgglomerativeClustering(n_clusters=4)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X)\n",
    "    labels = model.labels_\n",
    "    silhouette = silhouette_score(X, labels)\n",
    "    results[name] = silhouette\n",
    "\n",
    "# Compare results\n",
    "for name, silhouette in results.items():\n",
    "    print(f\"{name}: Silhouette Score = {silhouette:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Timeseries Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from fbprophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "# Example time series data (synthetic)\n",
    "dates = pd.date_range(start='2020-01-01', periods=100, freq='D')\n",
    "values = np.sin(np.linspace(0, 10, 100)) + np.random.normal(0, 0.1, 100)\n",
    "data = pd.DataFrame({'ds': dates, 'y': values})\n",
    "\n",
    "# Split data into train and test\n",
    "train = data.iloc[:80]\n",
    "test = data.iloc[80:]\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, train, test):\n",
    "    model.fit(train)\n",
    "    predictions = model.predict(test)\n",
    "    mae = mean_absolute_error(test['y'], predictions)\n",
    "    return mae\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'ARIMA': ARIMA(train['y'], order=(1, 1, 1)),\n",
    "    'SARIMA': SARIMAX(train['y'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12)),\n",
    "    'Prophet': Prophet(),\n",
    "    'LSTM': Sequential([\n",
    "        LSTM(50, activation='relu', input_shape=(1, 1)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    if name == 'LSTM':\n",
    "        # Reshape data for LSTM\n",
    "        X_train = train['y'].values.reshape(-1, 1, 1)\n",
    "        y_train = train['y'].values\n",
    "        X_test = test['y'].values.reshape(-1, 1, 1)\n",
    "        y_test = test['y'].values\n",
    "\n",
    "        # Train LSTM\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "        predictions = model.predict(X_test).flatten()\n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "    else:\n",
    "        mae = evaluate_model(model, train, test)\n",
    "    results[name] = mae\n",
    "\n",
    "# Compare results\n",
    "for name, mae in results.items():\n",
    "    print(f\"{name}: MAE = {mae:.4f}\")\n",
    "\n",
    "# Select the best model\n",
    "best_model = min(results, key=results.get)\n",
    "print(f\"Best Model: {best_model}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
