{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Data Integration\n",
    "Continuous Data Integration ensures that machine learning models are constantly updated with the latest data, reflecting current trends and behaviors in the real world. This process helps maintain model accuracy and relevance over time.\n",
    "\n",
    "Automated Data Pipelines: Automated data pipelines are essential for collecting, processing, and delivering data to the model training system in real time or at regular intervals.\n",
    "- Tools: Apache Kafka: A distributed streaming platform for real-time data ingestion.\n",
    "Apache NiFi: A dataflow automation tool for ingesting, transforming, and routing data.\n",
    "AWS Glue / Azure Data Factory: Cloud-based ETL services for building data pipeline.\n",
    " Apache Airflow can stream and process data in real time.\n",
    "- ETL (Extract, Transform, Load): Data is transformed before loading into the target system.ELT (Extract, Load, Transform): Data is loaded into the target system first, then transformed.\n",
    "\n",
    "Data Validation and Quality Checks:\n",
    "- Implement robust mechanisms to validate incoming data, ensuring it meets quality standards and is free from errors or anomalies.\n",
    "- Automated tests can check for missing values, out-of-range data, or format inconsistencies.\n",
    "- Use schema validation libraries like Great Expectations, Pydantic or TensorFlow Data Validation to enforce data contracts.\n",
    "- Implement automated alerts for data quality issues (e.g., using Prometheus or Grafana).\n",
    "- Log validation results for auditing and debugging.\n",
    "\n",
    "Dynamic Model Updates:\n",
    "- Integrate the data pipeline with the model retraining process or fine-tuned automatically, so that new validated data can trigger model retraining or updates dynamically.\n",
    "- Implement event-driven architecture to initiate model retraining based on data volume thresholds, schedule, or performance degradation.\n",
    "- This approach ensures that the model continuously evolves, adapting to recent changes without manual intervention. \n",
    "- Trigger retraining when new data is ingested or when data quality checks pass.\n",
    "- Use CI/CD tools (e.g., GitHub Actions, Jenkins) to automate the retraining process.\n",
    "\n",
    "\n",
    "#### Best Practices for Continuous Data Integration\n",
    "\n",
    "- Automate Everything:\n",
    "    - Use IaC (e.g., Terraform) to provision infrastructure for data pipelines.\n",
    "    - Use CI/CD to automate data validation, model retraining, and deployment.\n",
    "- Monitor Data Quality:\n",
    "    - Implement real-time monitoring for data pipelines (e.g., using Prometheus, Grafana).\n",
    "    - Set up alerts for data quality issues (e.g., missing values, anomalies).\n",
    "- Version Control:\n",
    "    - Use DVC (Data Version Control) or MLflow to track changes in data and models.\n",
    "    - Ensure reproducibility by versioning datasets, preprocessing steps, and models.\n",
    "- Scalability:\n",
    "    - Use distributed systems (e.g., Apache Kafka, Kubernetes) to handle large-scale data ingestion and processing.\n",
    "    - Optimize ETL/ELT pipelines for performance and scalability.\n",
    "\n",
    "\n",
    "#### Example Workflow: End-to-End Continuous Data Integration\n",
    "\n",
    "1) Data Ingestion: Use Apache Kafka to stream real-time data into the system.\n",
    "2) Data Validation: Validate incoming data using Python scripts or Great Expectations.\n",
    "3) Model Retraining: Trigger retraining using a CI/CD pipeline (e.g., GitHub Actions).\n",
    "4) Model Deployment: Deploy the updated model using Kubernetes or a cloud service (e.g., SageMaker).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Airflow DAG for Daily Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'ml_team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2023, 1, 1),\n",
    "    'email': ['ml-alerts@example.com'],\n",
    "    'email_on_failure': True,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "def validate_data_quality(**context):\n",
    "    # Connect to database and run validation checks\n",
    "    # Raise exception if quality thresholds aren't met\n",
    "    pass\n",
    "\n",
    "def check_data_drift(**context):\n",
    "    # Analyze statistical properties of new data\n",
    "    # Compare with baseline distribution\n",
    "    # Log drift metrics to monitoring system\n",
    "    pass\n",
    "\n",
    "with DAG('daily_model_data_integration',\n",
    "         default_args=default_args,\n",
    "         schedule_interval='0 2 * * *',  # Run daily at 2am\n",
    "         catchup=False) as dag:\n",
    "    \n",
    "    # Extract data from production sources\n",
    "    extract_new_data = S3ToRedshiftOperator(\n",
    "        task_id='extract_new_data',\n",
    "        schema='ml_pipeline',\n",
    "        table='daily_raw_data',\n",
    "        s3_bucket='production-events',\n",
    "        s3_key='events/{{ ds }}/',\n",
    "        redshift_conn_id='redshift_default'\n",
    "    )\n",
    "    \n",
    "    # Validate data quality\n",
    "    quality_check = PythonOperator(\n",
    "        task_id='validate_data_quality',\n",
    "        python_callable=validate_data_quality\n",
    "    )\n",
    "    \n",
    "    # Check for data drift\n",
    "    drift_check = PythonOperator(\n",
    "        task_id='check_data_drift',\n",
    "        python_callable=check_data_drift\n",
    "    )\n",
    "    \n",
    "    # Additional tasks for feature engineering, etc.\n",
    "    \n",
    "    # Define workflow\n",
    "    extract_new_data >> quality_check >> drift_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-Time Data Ingestion with Apache Kafka\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Set up Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'data_topic',  # Topic to consume from\n",
    "    bootstrap_servers='localhost:9092',  # Kafka server address\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))  # Deserialize JSON data\n",
    "\n",
    "# Process incoming data\n",
    "for message in consumer:\n",
    "    data = message.value\n",
    "    print(f\"Received data: {data}\")\n",
    "    # Add data processing logic here (e.g., cleaning, transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ETL Pipeline with Python\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Extract data from a CSV file\n",
    "data = pd.read_csv('data_source.csv')\n",
    "\n",
    "# Transform data (e.g., clean missing values, normalize columns)\n",
    "data['column'] = data['column'].fillna(data['column'].mean())  # Fill missing values\n",
    "data['normalized_column'] = (data['column'] - data['column'].mean()) / data['column'].std()  # Normalize\n",
    "\n",
    "# Load data into a database\n",
    "engine = create_engine('postgresql://user:password@localhost:5432/mydatabase')\n",
    "data.to_sql('processed_data', engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation and Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "\n",
    "def validate_new_data(data_path):\n",
    "    # Load the data into a Great Expectations DataFrame\n",
    "    df = ge.read_csv(data_path)\n",
    "    \n",
    "    # Define expectations for the data\n",
    "    validation_results = df.expect_column_values_to_not_be_null(\"user_id\")\n",
    "    validation_results &= df.expect_column_values_to_be_between(\"age\", min_value=18, max_value=120)\n",
    "    validation_results &= df.expect_column_values_to_be_in_set(\"subscription_type\", [\"free\", \"basic\", \"premium\"])\n",
    "    validation_results &= df.expect_column_mean_to_be_between(\"daily_usage_minutes\", min_value=5, max_value=300)\n",
    "    \n",
    "    # Generate a validation report\n",
    "    validation_report = df.validate()\n",
    "    \n",
    "    if not validation_report.success:\n",
    "        # Log the issues and potentially halt the pipeline\n",
    "        raise ValueError(f\"Data validation failed: {validation_report.get_failed_expectations()}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Validation with Python\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('data_source.csv')\n",
    "\n",
    "# Validate data\n",
    "def validate_data(data):\n",
    "    # Check for missing values\n",
    "    if data.isnull().any().any():\n",
    "        raise ValueError(\"Data contains missing values.\")\n",
    "    \n",
    "    # Check for out-of-range values\n",
    "    if (data['column'] < 0).any():\n",
    "        raise ValueError(\"Data contains negative values in 'column'.\")\n",
    "    \n",
    "    # Check for format inconsistencies\n",
    "    if not data['date_column'].dtype == 'datetime64[ns]':\n",
    "        raise ValueError(\"Date column is not in the correct format.\")\n",
    "\n",
    "# Run validation\n",
    "validate_data(data)\n",
    "print(\"Data validation passed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Model Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from datetime import datetime\n",
    "\n",
    "def retrain_model_if_needed(performance_metrics, data_drift_metrics, \n",
    "                            retraining_threshold=0.03, model_id=\"production-model-v1\"):\n",
    "    \"\"\"\n",
    "    Conditionally retrain model based on performance or drift metrics\n",
    "    \"\"\"\n",
    "    # Check if metrics indicate need for retraining\n",
    "    if (performance_metrics['accuracy_drop'] > retraining_threshold or \n",
    "        data_drift_metrics['feature_drift_score'] > retraining_threshold):\n",
    "        \n",
    "        # Load latest validated data\n",
    "        X_train, y_train = load_latest_training_data()\n",
    "        X_valid, y_valid = load_latest_validation_data()\n",
    "        \n",
    "        # Start MLflow run for tracking\n",
    "        with mlflow.start_run(run_name=f\"auto_retrain_{datetime.now().strftime('%Y%m%d_%H%M')}\"):\n",
    "            # Train new model\n",
    "            model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate on validation data\n",
    "            validation_score = model.score(X_valid, y_valid)\n",
    "            mlflow.log_metric(\"validation_accuracy\", validation_score)\n",
    "            \n",
    "            # Log other relevant metrics\n",
    "            mlflow.log_params({\n",
    "                \"retrain_reason\": \"performance_drop\" if performance_metrics['accuracy_drop'] > retraining_threshold \n",
    "                                 else \"data_drift\",\n",
    "                \"drift_score\": data_drift_metrics['feature_drift_score'],\n",
    "                \"accuracy_drop\": performance_metrics['accuracy_drop']\n",
    "            })\n",
    "            \n",
    "            # Register new model if better than current one\n",
    "            if validation_score > performance_metrics['current_accuracy']:\n",
    "                mlflow.sklearn.log_model(model, \"model\")\n",
    "                model_version = mlflow.register_model(f\"runs:/{mlflow.active_run().info.run_id}/model\", \n",
    "                                                     model_id)\n",
    "                \n",
    "                # Transition to staging\n",
    "                client = mlflow.tracking.MlflowClient()\n",
    "                client.transition_model_version_stage(\n",
    "                    name=model_id,\n",
    "                    version=model_version.version,\n",
    "                    stage=\"Staging\"\n",
    "                )\n",
    "                \n",
    "                return True, model_version.version\n",
    "    \n",
    "    return False, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triggering Model Retraining\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Load new data\n",
    "new_data = pd.read_csv('new_data.csv')\n",
    "\n",
    "# Validate new data\n",
    "validate_data(new_data)\n",
    "\n",
    "# Load existing model\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "# Retrain model with new data\n",
    "X_new = new_data.drop('target', axis=1)\n",
    "y_new = new_data['target']\n",
    "model.fit(X_new, y_new)\n",
    "\n",
    "# Save updated model\n",
    "joblib.dump(model, 'model.pkl')\n",
    "print(\"Model retrained and updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub Actions Workflow for Retraining\n",
    "\n",
    "name: Model Retraining Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches:\n",
    "      - main\n",
    "  schedule:\n",
    "    - cron: '0 0 * * *'  # Daily retraining\n",
    "\n",
    "jobs:\n",
    "  retrain:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v2\n",
    "\n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v2\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "\n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "\n",
    "      - name: Run data validation\n",
    "        run: python validate_data.py\n",
    "\n",
    "      - name: Retrain model\n",
    "        run: python retrain_model.py\n",
    "\n",
    "      - name: Deploy updated model\n",
    "        run: python deploy_model.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
